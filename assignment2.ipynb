{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age  SystolicBP  DiastolicBP    BS  BodyTemp  HeartRate  RiskLevel\n",
      "0   25         130           80  15.0      98.0         86  high risk\n",
      "1   35         140           90  13.0      98.0         70  high risk\n",
      "2   29          90           70   8.0     100.0         80  high risk\n",
      "3   30         140           85   7.0      98.0         70  high risk\n",
      "4   35         120           60   6.1      98.0         76   low risk\n"
     ]
    }
   ],
   "source": [
    "dataset_url = \"https://raw.githubusercontent.com/ZachLegros/CSI4106-A2-Classification/master/Maternal%20Health%20Risk%20Data%20Set.csv\"\n",
    "df = pd.read_csv(dataset_url)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "LogisticRegression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    low risk       0.73      0.93      0.82       105\n",
      "    mid risk       0.45      0.41      0.43        64\n",
      "   high risk       0.89      0.65      0.75        85\n",
      "\n",
      "    accuracy                           0.70       254\n",
      "   macro avg       0.69      0.66      0.66       254\n",
      "weighted avg       0.71      0.70      0.70       254\n",
      "\n",
      "GaussianNB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    low risk       0.61      0.88      0.72       105\n",
      "    mid risk       0.35      0.22      0.27        64\n",
      "   high risk       0.92      0.67      0.78        85\n",
      "\n",
      "    accuracy                           0.64       254\n",
      "   macro avg       0.62      0.59      0.59       254\n",
      "weighted avg       0.65      0.64      0.62       254\n",
      "\n",
      "------------------------------------------------------\n",
      "Fold 2\n",
      "LogisticRegression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    low risk       0.67      0.66      0.66       122\n",
      "    mid risk       0.25      0.30      0.27        61\n",
      "   high risk       0.77      0.68      0.72        71\n",
      "\n",
      "    accuracy                           0.57       254\n",
      "   macro avg       0.56      0.54      0.55       254\n",
      "weighted avg       0.60      0.57      0.59       254\n",
      "\n",
      "GaussianNB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    low risk       0.68      0.89      0.77       122\n",
      "    mid risk       0.23      0.13      0.17        61\n",
      "   high risk       0.81      0.68      0.74        71\n",
      "\n",
      "    accuracy                           0.65       254\n",
      "   macro avg       0.57      0.56      0.56       254\n",
      "weighted avg       0.61      0.65      0.61       254\n",
      "\n",
      "------------------------------------------------------\n",
      "Fold 3\n",
      "LogisticRegression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    low risk       0.65      0.71      0.68       101\n",
      "    mid risk       0.53      0.45      0.49        95\n",
      "   high risk       0.68      0.74      0.71        57\n",
      "\n",
      "    accuracy                           0.62       253\n",
      "   macro avg       0.62      0.63      0.63       253\n",
      "weighted avg       0.61      0.62      0.61       253\n",
      "\n",
      "GaussianNB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    low risk       0.56      0.96      0.71       101\n",
      "    mid risk       0.68      0.24      0.36        95\n",
      "   high risk       0.80      0.65      0.72        57\n",
      "\n",
      "    accuracy                           0.62       253\n",
      "   macro avg       0.68      0.62      0.59       253\n",
      "weighted avg       0.66      0.62      0.58       253\n",
      "\n",
      "------------------------------------------------------\n",
      "Fold 4\n",
      "LogisticRegression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    low risk       0.47      0.76      0.58        78\n",
      "    mid risk       0.57      0.34      0.42       116\n",
      "   high risk       0.72      0.73      0.72        59\n",
      "\n",
      "    accuracy                           0.56       253\n",
      "   macro avg       0.59      0.61      0.58       253\n",
      "weighted avg       0.58      0.56      0.54       253\n",
      "\n",
      "GaussianNB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    low risk       0.41      0.79      0.54        78\n",
      "    mid risk       0.43      0.20      0.27       116\n",
      "   high risk       0.76      0.63      0.69        59\n",
      "\n",
      "    accuracy                           0.48       253\n",
      "   macro avg       0.53      0.54      0.50       253\n",
      "weighted avg       0.50      0.48      0.45       253\n",
      "\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Here we get each row of the dataset and remove the last column (the class). This is our X data and it is a 2D array.\n",
    "X = df.iloc[:, :-1].values\n",
    "# Here we get the last column of the dataset. This is our y data.\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "kf = KFold(n_splits=4)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "  print(\"Fold\", i+1)\n",
    "  # use classification_report to generate a report with precision and recall only and compare the micro and macro averages\n",
    "  X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "  clf = LogisticRegression(random_state=0, max_iter=1500)\n",
    "  clf.fit(X_train, y_train)\n",
    "  clf_ypred= clf.predict(X_test)\n",
    "  clf_report = classification_report(y_test, clf_ypred, labels=[\"low risk\", \"mid risk\", \"high risk\"])\n",
    "  print(\"LogisticRegression:\")\n",
    "  print(clf_report)\n",
    "\n",
    "  gnb = GaussianNB()\n",
    "  gnb.fit(X_train, y_train)\n",
    "  gnb_ypred = gnb.predict(X_test)\n",
    "  gnb_report = classification_report(y_test, gnb_ypred, labels=[\"low risk\", \"mid risk\", \"high risk\"])\n",
    "  print(\"GaussianNB:\")\n",
    "  print(gnb_report)\n",
    "  print(\"------------------------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
